# ADR-002: 실시간 파이프라인 지연 최소화 전략

**상태**: 승인됨 (Accepted)
**일자**: 2025-12-10
**카테고리**: 충돌 해결 (속도 A vs 외부 API 지연)
**작성자**: DNA Developer

---

## 1. 맥락 (Context)

본 프로젝트(AI Moderator)는 사용자의 발언을 실시간으로 분석하여 중재안을 제시해야 합니다.
**NFR 목표는 '발언 후 1초 이내 반응'**이나, 다음과 같은 현실적 제약(Latency Budget)이 존재합니다:

1.  **Google Cloud STT**: 음성 패킷 전송 → 인식 → 텍스트 반환 (약 300~500ms 소요)
2.  **Gemini API**: 프롬프트 전송 → 추론 시작 → 첫 토큰 생성 (약 500~1,500ms 소요)
3.  **네트워크**: 클라이언트 ↔ 서버 왕복 (약 50~100ms)

단순 순차 처리(Sequential Processing) 시 **총 지연시간이 2초 이상** 발생하여, 실시간성이 생명인 "중재자" 역할을 수행하기 어렵습니다.

---

## 2. 검토한 대안 (Alternatives)

### 대안 1: 순차적 처리 (Sequential) - "완성된 문장만 처리"
* **방식**: 사용자의 문장이 끝날 때까지 기다림(STT 완료) → 완성된 텍스트를 Gemini에 전송 → Gemini 답변 완료 대기 → 클라이언트에 전송.
* **장점**: 구현이 매우 단순함. 문맥이 완벽한 상태에서 AI가 분석하므로 정확도 높음.
* **단점**: **치명적 지연(Latency > 2s)**. 사용자가 이미 다음 화제로 넘어간 뒤에 뒷북 중재 발생.
* **판단**: ❌ 기각 (대회 입상 불가 수준의 UX)

### 대안 2: 로컬 경량 모델 (Local LLM/STT) - "인프라 힘으로 해결"
* **방식**: 서버에 Whisper(STT)와 Llama-3-8B(LLM)를 직접 띄워 네트워크 홉을 제거.
* **장점**: 네트워크 지연 최소화, 보안성 우수.
* **단점**: 고사양 GPU 서버 필요(비용/관리 부담), 설치 및 최적화 시간 과다 소요.
* **판단**: ❌ 기각 ("인프라 관리 최소화" 제약 위배)

### 대안 3: 완전 스트리밍 파이프라인 (Streaming Pipeline) - "낙관적 처리"
* **방식**:
    1.  **STT 스트리밍**: 문장이 완성되지 않아도 '중간 인식 결과(Interim Results)'를 즉시 클라이언트에 전송하여 화면에 표시.
    2.  **LLM 스트리밍**: 문장이 완성되는 즉시(Final Result) Gemini에 요청하되, Gemini의 응답을 기다리지 않고 **생성되는 토큰 단위(Character-by-character)**로 WebSocket을 통해 클라이언트에 즉시 푸시.
    3.  **병렬 처리**: STT가 다음 문장을 인식하는 동안, 백그라운드에서 이전 문장의 AI 분석 수행.
* **장점**: **체감 지연시간(Perceived Latency) < 500ms**. 사용자는 발언 즉시 화면 반응을 봄.
* **단점**: 구현 복잡도 증가(비동기 상태 관리, WebSocket 스트림 제어).

---

## 3. 결정 (Decision)

**대안 3 (완전 스트리밍 파이프라인)**을 채택합니다.

이를 위해 Python **FastAPI의 `asyncio`**를 기반으로 한 **Non-blocking Event Loop** 아키텍처를 구현하며, 모든 외부 통신(STT, Gemini)은 **Stream 모드**를 강제합니다.

### 핵심 구현 전략
1.  **WebSocket Manager**: 클라이언트 연결 하나당 `Receiver Task`(음성 수신)와 `Sender Task`(결과 전송)를 분리하여 병렬 처리.
2.  **Optimistic UI**: STT의 `is_final=False` 데이터도 화면에 회색 텍스트로 즉시 렌더링하여 "듣고 있음"을 시각적으로 피드백.
3.  **Gemini Stream**: `response.resolve()` 대신 `response_stream`을 사용하여 첫 토큰 생성 즉시 전송 시작.

---

## 4. 결과 (Consequences)

### 긍정적 영향 (Benefits)
* ✅ **경쟁 우위 확보**: 기술적으로 가장 난이도 높은 비동기 스트리밍을 구현함으로써 대회 심사에서 고득점 요소 확보.
* ✅ **UX 혁신**: 사용자는 AI가 "실시간으로 생각하고 있다"는 느낌을 받음.
* ✅ **리소스 효율**: 요청을 대기하는 동안 스레드가 차단(Block)되지 않아 단일 서버로도 높은 동시성 처리 가능.

### 부정적 영향 및 대응 (Trade-offs)
* ⚠️ **복잡성**: WebSocket 연결이 불안정할 경우 재연결 및 상태 복구 로직이 필요함.
    * → *대응*: 간단한 클라이언트 측 재연결(Retry) 로직만 구현하고, 서버 상태는 Stateless에 가깝게 설계하여 복잡도 억제.
* ⚠️ **토큰 비용**: 스트리밍 중 취소(발언 중단 등)가 어려워 불필요한 토큰이 소모될 수 있음.
    * → *대응*: 대회용이므로 비용 효율성(NFR 순위 낮음)보다 속도를 우선시하여 감수함.

---

## 5. 준수 사항 (Compliance)

* **검증 방법**: `Stage 9` 체크리스트에 "Latency 측정 테스트" 포함. (발언 종료 후 첫 번째 AI 응답 토큰 도착까지의 시간 측정)
* **강제 규칙**: `PROJECT_STANDARDS.md`에 **"모든 I/O 함수는 `async def`로 선언해야 하며, `time.sleep()` 등의 동기 함수 사용을 금지한다"** 규칙 추가.